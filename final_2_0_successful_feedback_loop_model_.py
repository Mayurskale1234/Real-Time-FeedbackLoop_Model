
"""Final 2.0 Successful FeedBack loop Model .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VPyR6AqORalSHb1qcedVIhoy2ayNs9fp
"""

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score
from transformers import RobertaTokenizer, RobertaForQuestionAnswering, AdamW, get_linear_schedule_with_warmup
import torch
from torch.utils.data import DataLoader, TensorDataset
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

# RoBERTa model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")
qa_model = AutoModelForQuestionAnswering.from_pretrained("deepset/roberta-base-squad2")

def fine_tune_roberta(model, tokenizer, train_data, epochs=3, batch_size=96, lr=3e-5):
    optimizer = AdamW(model.parameters(), lr=lr)

    # Encodeing training data
    train_inputs = []
    train_labels = []

    for q, c, (start, end) in train_data:
        encoded = tokenizer.encode_plus(q, c, truncation=True, padding='max_length', max_length=512)
        train_inputs.append(encoded)
        train_labels.append((start, end))

    # Creating TensorDataset
    train_dataset = TensorDataset(
        torch.tensor([t['input_ids'] for t in train_inputs], dtype=torch.long),
        torch.tensor([t['attention_mask'] for t in train_inputs], dtype=torch.long),
        torch.tensor([l[0] for l in train_labels], dtype=torch.long),
        torch.tensor([l[1] for l in train_labels], dtype=torch.long)
    )

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    total_steps = len(train_loader) * epochs
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

    model.train()

    for epoch in range(epochs):
        total_loss = 0
        for batch in train_loader:
            input_ids, attention_mask, start_positions, end_positions = batch
            input_ids = input_ids.to(model.device)
            attention_mask = attention_mask.to(model.device)
            start_positions = start_positions.to(model.device)
            end_positions = end_positions.to(model.device)

            model.zero_grad()

            outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)
            loss = outputs.loss
            total_loss += loss.item()
            loss.backward()
            optimizer.step()
            scheduler.step()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch+1}/{epochs} - Loss: {avg_loss}")

def get_roberta_answer(model, tokenizer, question, context):
    inputs = tokenizer.encode_plus(question, context, return_tensors='pt', truncation=True, padding=True)
    inputs = {key: value.to(model.device) for key, value in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)

    start_logits = outputs.start_logits
    end_logits = outputs.end_logits

   # if answer_start >= answer_end:
    #    return "No valid answer found."

    # Debug logs
    print(f"Question: {question}")
    print(f"Context: {context}")
    print(f"Start logits: {start_logits}")
    print(f"End logits: {end_logits}")

    answer_start = torch.argmax(start_logits, dim=1).item()
    answer_end = torch.argmax(end_logits, dim=1).item() + 1

    # debug logs
    print(f"Answer start index: {answer_start}")
    print(f"Answer end index: {answer_end}")


    answer_tokens = inputs['input_ids'][0][answer_start:answer_end]
    answer_tokens = tokenizer.convert_ids_to_tokens(answer_tokens, skip_special_tokens=True)
    answer = tokenizer.convert_tokens_to_string(answer_tokens)

    # Final answer debug log
    print(f"Answer tokens: {answer_tokens}")
    print(f"Answer: {answer}")


    return answer

# Example data
train_data = [
    ("What is the capital of France?", "France is a country in Europe. The capital of France is Paris.", (9, 10)),
    ("Where is the Eiffel Tower located?", "The Eiffel Tower is located in Paris, France.", (6, 6)),
    # can add more data if needed.
]


fine_tune_roberta(qa_model, tokenizer, train_data)

class OnlineLearningFeedbackModel:
    def __init__(self, classes=np.array([0, 1])):
        self.vectorizer = TfidfVectorizer()
        self.model = SGDClassifier()
        self.classes = classes

    def fit_initial_model(self, data):
        texts = [q + " " + a for q, a, label in data]
        X = self.vectorizer.fit_transform(texts)
        y = np.array([label for q, a, label in data])
        self.model.partial_fit(X, y, classes=self.classes)

    def update_model(self, new_data):
        texts = [q + " " + a for q, a, label in new_data]
        new_X = self.vectorizer.transform(texts)
        new_y = np.array([label for q, a, label in new_data])
        self.model.partial_fit(new_X, new_y, classes=self.classes)

    def evaluate_model(self, validation_data):
        texts = [q + " " + a for q, a, label in validation_data]
        val_X = self.vectorizer.transform(texts)
        val_y = np.array([label for q, a, label in validation_data])
        predictions = self.model.predict(val_X)
        accuracy = accuracy_score(val_y, predictions)
        return accuracy

initial_data = [
    ("What is AI?", "AI stands for artificial intelligence.", 1),
    ("What is ML?", "ML stands for machine learning.", 1),
    ("What is the capital of France?", "The capital of France is Paris.", 1),
    ("What is the largest ocean?", "The largest ocean is the Pacific Ocean.", 1)
]

feedback_data = [
    ("What is the capital of France?", "France is a country in Europe. The capital of France is Paris.", (10, 15)),
    ("What is AI?", "AI stands for artificial intelligence.", (0, 3))

]

feedback_model = OnlineLearningFeedbackModel()
feedback_model.fit_initial_model(initial_data)

new_data = [
    ("What is Python?", "Python is a programming language.", 1),
    ("What is the speed of light?", "The speed of light is approximately 299,792 kilometers per second.", 1)
]
feedback_model.update_model(new_data)

validation_data = [
    ("What is AI?", "AI stands for artificial intelligence.", 1),
    ("What is Python?", "Python is a programming language.", 1),
    ("What is the speed of light?", "The speed of light is approximately 299,792 kilometers per second.", 1)
]

accuracy = feedback_model.evaluate_model(validation_data)
print(f"Model accuracy: {accuracy}")

def simulate_user_interaction_and_collect_feedback():
    while True:
        question = input("Ask a question (type 'exit' to end): ")
        if question.lower() == 'exit':
            break
        context = input("Provide some context for the question: ")


        answer = get_roberta_answer(question=question, context=context, model=qa_model, tokenizer=tokenizer)
        print(f"RoBERTa's answer: {answer}")

        feedback = input("Was RoBERTa's answer correct? (yes/no): ").strip().lower()
        feedback_label = 1 if feedback == "yes" else 0
        feedback_data = [(question, context, feedback_label)]

        # Updating feedback model
        feedback_model.update_model(feedback_data)
        print("Feedback loop model updated with user feedback.")


        feedback_data = [(question, context, (0, 0, feedback_label))]  # Using (0, 0) as placeholders


simulate_user_interaction_and_collect_feedback()

accuracy_after_feedback = feedback_model.evaluate_model(validation_data)
print(f'Validation Accuracy after user feedback: {accuracy_after_feedback}')